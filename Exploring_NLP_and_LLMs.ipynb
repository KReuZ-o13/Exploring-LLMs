{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring NLPs and LLMs\n",
        "Hello! This is supposed to be a demo for the types of tasks you might run into and show you some of the tools you can use for the tasks, with both traditional NLP methods and LLMs! This is a taster - there are many more types of tasks, and even more ways in which you can complete them!\n",
        "This will  focus on open source models and datasets that we can easily access from HuggingFace (HF)!"
      ],
      "metadata": {
        "id": "VF2vQZ7bOBwJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Sentiment analysis**\n",
        "You can use traditional NLP methods to gauge the sentiment present in the text. Classic tools such as VADER and Textblob are light weight and fast making them a godsend for big datasets, but struggle with longer sentences and nuance, such as when sarcasm is used. On the other hand, NLP models such as the ones used here might take more time, but offer can offer better classification of the text and might be beneficial for text that might not be as straightforward. However, remember that the hallucinations are with you - before running this on a large dataset, manually evaluate the results on a small one."
      ],
      "metadata": {
        "id": "-IchLlogmC82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Pip install the packages we need\n",
        "#Because of course they don't come on vanilla colab\n",
        "!pip install vaderSentiment\n",
        "!pip install transformers\n",
        "!pip install textstat\n",
        "!pip install textblob"
      ],
      "metadata": {
        "collapsed": true,
        "id": "j80X8w3vOBjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EFunl-w2_zj"
      },
      "outputs": [],
      "source": [
        "#Load in the packages\n",
        "#Some stat and data handling packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as stats\n",
        "\n",
        "#A whole host of NLP and HF packages\n",
        "import re  #This is supposed to help with regular expressions. Unique symbols? Arcane knowledge? Someone who keeps having newlines in their text? Use re to clean the text!\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer #This is the VADER sentiment analyser - easy to use, 10/10 elite employee\n",
        "from transformers import pipeline #Very important for using hugging face models.\n",
        "import textstat #Do you need more information about the word count? Readability scores (in English)? This has got you fam.\n",
        "from textblob import TextBlob #This can also give you the sentiment, but has an additional component! You can look at the subjetcivity of the text!\n",
        "import nltk #This is the all-in-one traditional NLP toolkit. Want to do anything? It's got you. That said, I don't like using it lol. It's here to show you some of the stuff you can use\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download resources for the nltk packages\n",
        "# We're proably not going to use them, but I'm always paranoid about leaving it out when I analyse text\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "4nPMcSJSOGVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a random dataset - you can change this up to see how the different scores you get look!\n",
        "#This is a list. You list things out in a list\n",
        "reviews = ['Man, the food here was nasty. Would not recommend.',\n",
        "             'I absolutely loved everything on the menu!',\n",
        "             'It was fine, I guess.',\n",
        "             'Bad food, poor service.',\n",
        "             'Mediocre at best, unappetizing at its worst.',\n",
        "             'That was some really amazing pasta!',\n",
        "             'The tiramisu was delicious. Good service, recommended the restaurant to my friends!',\n",
        "             'Lovely ambience, terrible food.']"
      ],
      "metadata": {
        "id": "fKHgtyPGyK2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#As I am allergic to list, I am converting it into a dataframe. All hail pandas\n",
        "sent_text = pd.DataFrame(reviews, columns=['Reviews'])"
      ],
      "metadata": {
        "id": "N-b9HMuhUhtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check\n",
        "sent_text.head(10)"
      ],
      "metadata": {
        "id": "n1gXGFDJPBI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Okay, we have our food reviews for this totally existing Italian place\n",
        "#Now i want you to add a column with the 'ground truth'\n",
        "#Or rather the vibe the review is giving\n",
        "#Is it positive?\n",
        "#Negative?\n",
        "#Neutral?\n",
        "sent_text['Ground Truth'] = ['vibe1', 'vibe2', 'vibe3', 'vibe4',\n",
        "                             'vibe5', 'vibe6', 'vibe7', 'vibe8']"
      ],
      "metadata": {
        "id": "2aP4FXgWGSiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The data needs cleaning and normalisation\n",
        "#Cleaning the text data from both to remove the weird spaces and stuff\n",
        "#Having it as a function let's you come back and add more bizarre conditions depeding on how your data looks like\n",
        "def clean_text(text):\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    # Remove underscores\n",
        "    text = text.replace('_', '')\n",
        "    # Remove special characters and numbers but keep ending punctuation\n",
        "    text = re.sub(r\"[^a-z0-9\\s.,!?;:'\\\"()\\[\\]{}\\-]\", '', text)\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    #Remove any jumps\n",
        "    text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
        "    return text"
      ],
      "metadata": {
        "id": "R5gRhhSqUPWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Apply the function\n",
        "sent_text['Reviews'] = sent_text['Reviews'].apply(clean_text)"
      ],
      "metadata": {
        "id": "aMVSmU5tVoNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### THe DaRTH VADER edit\n",
        "Well, it's just VADER. This is one of the (very many) lightweight models that analyses informal, short form content and provides continuous numerical scores from -1 to 1 which can essentially be gruped into 3 ranges - positive, negative and neutral."
      ],
      "metadata": {
        "id": "cbe_bxDTJkiz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VADER analysis!\n",
        "#Initial vader set up\n",
        "vader = SentimentIntensityAnalyzer()\n",
        "#Then we find out what VADER scores them - We get a score of the sentence from -1 to 1\n",
        "sent_text['vader_compound'] = sent_text['Reviews'].apply(lambda x: vader.polarity_scores(x)['compound'])\n",
        "#But reading the values directly is annoying - so we group them into 3,  +ve, neutral, -ve\n",
        "sent_text['vader_sentiment'] = sent_text['vader_compound'].apply(lambda x: 'positive' if x >= 0.5 else ('negative' if x <= -0.5 else 'neutral'))"
      ],
      "metadata": {
        "id": "BvG3__9jOxR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Then we look at it:\n",
        "sent_text.head()"
      ],
      "metadata": {
        "id": "FL0YKLg9hiIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Okay, time for the fun bit, more visualisations!\n",
        "#Let's see how many there are in each category\n",
        "f,ax = plt.subplots(figsize=(8,6))\n",
        "\n",
        "#Primary enrollment as per the census\n",
        "sns.countplot(data = sent_text, ax = ax, x = 'vader_sentiment', palette = 'viridis')\n",
        "\n",
        "# Labeling\n",
        "ax.set_title('Sentiment of the food reviews with VADER')\n",
        "ax.set_ylabel('Number of reviews')\n",
        "ax.set_xlabel('Categories')"
      ],
      "metadata": {
        "id": "DNmc8gPojE6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The Sesame Street BERT edit\n",
        "\n",
        "Okay, we've tried that with VADER! Now let's see how it looks like with an LLM! For this sectiojn we will be using NLPTown's bert-base-multilingual-uncased-sentiment model, which we will be getting from HuggingFace! This model has been fine-tuned on reviews and can give you the scores from one to five stars!"
      ],
      "metadata": {
        "id": "W8fvhOMrk-5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Setting up the LLM for the portion!\n",
        "#First, we load pre-trained BERT pipeline\n",
        "bert_pipeline = pipeline('sentiment-analysis', model='nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "\n",
        "# Map BERT labels to positive, negative and neutral!\n",
        "star_to_sentiment = {'1 star': 'negative', '2 stars': 'negative', '3 stars': 'neutral',\n",
        "                     '4 stars': 'positive', '5 stars': 'positive'}"
      ],
      "metadata": {
        "id": "YRNhojUuk-Sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We can then create a function to do the classifying for us here!\n",
        "#Create a BERT classifier function\n",
        "def classify_bert(text):\n",
        "    try:\n",
        "        result = bert_pipeline(text[:512])[0]  # this returns the top predictions, at the maximum input length\n",
        "        return star_to_sentiment[result['label']] #This converts the stars to our labels\n",
        "    except:\n",
        "        return 'error' #This catches any problems before everything crashes and burns and takes out weeks of work."
      ],
      "metadata": {
        "id": "_ymtmm8Zm-Mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Classify the reviews\n",
        "sent_text['bert_sentiment'] = sent_text['Reviews'].apply(classify_bert)"
      ],
      "metadata": {
        "id": "dD38sWsFnKJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Okay, time for the fun bit, more visualisations!\n",
        "#Let's see how many there are in each category\n",
        "f,ax = plt.subplots(figsize=(8,6))\n",
        "\n",
        "#Primary enrollment as per the census\n",
        "sns.countplot(data = sent_text, ax = ax, x = 'bert_sentiment', palette = 'viridis')\n",
        "\n",
        "# Labeling\n",
        "ax.set_title('Sentiment of the food reviews for BERT')\n",
        "ax.set_ylabel('Number of reviews')\n",
        "ax.set_xlabel('Categories')"
      ],
      "metadata": {
        "id": "vsy4q7hPoyHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Okay, let's see them all\n",
        "sent_text.head(10)"
      ],
      "metadata": {
        "id": "FcY9LZPBu9aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, some of the phrases we have have been classified differently. BERT picked up the negative sentiment in 'Mediocre at best, unappetizing at its worst.', but misclassified 'That was some really amazing pasta!' and 'Lovely ambience, terrible food.' That said, this is the first time I've run this, and BERT likes reclassifying things, so it might look different. Who knows.\n",
        "\n",
        "We can also compare the tool performance by comparing it to the ground truth you just added to the reviews before we ran everything!\n",
        "\n",
        "\n",
        "Therefore remember, when you're using these techniques!!! To always check on a sample of your work!! And pick the right technique for the right problem!!!!\n",
        "\n",
        "Also an advantage of LLMs like BERT: They can process things like emojis.\n",
        "\n",
        "Now, you can change up the reviews! Add more reviews, change the words and add five emojis. See how the sentiment changes, and add your own spin!"
      ],
      "metadata": {
        "id": "KrL14adppDkV"
      }
    }
  ]
}